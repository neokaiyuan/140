			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Devon Hinton <dhinton@stanford.edu>
Peter Hu <peterhu@stanford.edu>
Kai-Yuan Neo <kneo@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Devon Hinton: AC - 1/3;
Peter Hu: AC - 1/3;
Kai-Yuan Neo: AC - 1/3;

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

#1: struct thread
struct thread
  {
		.
		.
		.
    int64_t ticksToWake;                /* stores the time at which to wake */
    struct list_elem readyElem;         /* Used for normal and mlfqs ready_lists */
    struct list_elem timerWaitElem;     /* Used for wait_list in timer functions */
		.
		.
		.
  };

Purpose: The modified thread struct contains information necessary for the
waking of a thread after timer_sleep has been called. It contains the 
absolute tick number after which it should be woken, and the list_elem to
store this thread in the wait_list. We also changed name of the list_elem
readyElem from elem in order to make clearer its purpose.

#2: list wait_list
/* list that keeps track of waiting threads */
static struct list wait_list

Purpose: This is the list that contains thread structs in sorted
order that we look at every system tick to check if any
threads need to be woken.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

After timer_sleep is called, we first disable interrupts to ensure that the
interrupt handler does not mess us up. We then calculate the time at which 
this thread would want to be woken up by adding the ticks argument to the 
current absolute ticks, and then calling our helper function thread_sleep, 
implemented in thread.c. thread_sleep initializes a thread struct and populates 
it with a pointer to the current thread and stores the release time (in ticks). 
It then inserts that struct into our wait_list, maintaining the sorted order 
of the list, before blocking the thread. Interrupts are enabled right after
blocking the thread.

When the timer interrupt handler is called, the absolute ticks count is
incremented, and then our helper function thread_wake_all_ready is called (defined
in thread.c). thread_wake_all_ready takes in the argument of the number of ticks,
and iterates through the wait list, waking up every thread it encounters until
it reaches a thread that does not need to be woken up (does not wake this one).
Each thread that is woken up is removed from the wait_list and subsequently
unblocked.

Back in the timer interrupt function, after thread_wake_all_ready returns,
thread_tick is called. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We maintain wait_list as a sorted linked list to ensure that 
thread_wake_all_ready only checks the number of threads it needs to wake plus one.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

There is no shared memory access in timer_sleep, however the wait_list 
variable is shared in thread_sleep, which is called by timer_sleep. Thus we 
disable interrupts at the beginning of the thread_sleep function and wake it up
at the end in order to prevent messing up the wait_list.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Again, there is no shared memory accessed in timer_sleep, only in thread_sleep
which is called from timer_sleep. In thread_sleep we have disabled interrupts
in the beginning, which prevents any race conditions with regard to the
shared memory wait_list.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

This design is modular and hierarchical. It makes sense that the functions
that deal with sleeping and waking threads are written in thread.c, and the
code that deals with incrementing and passing tick counts are written in
timer.c.

Another design we considered was to write all our functions in timer.c as well
as to declare our modified thread struct and wait_list there (statically). Thus
while the intratracies of blocking/unblocking threads would be dealt with in
module thread.c, access to which threads where blocked (via sleep) would 
be accessible in timer.c, via the wait_list (and thread structs there
contained). One could imagine a situation where a thread must be unblocked yet
no access is available to it, due to this violation of modularity. It does not 
make sense to store threads in one file (timer.c) and access them directly in 
another (thread.c).

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

#1:
In the semaphore_elem in sync.c:
struct semaphore_elem {
  struct list_elem elem;
  struct semaphore semaphore;
  int priority;                   ------------------------------ADDED
}

Int priority is an optimization to wake the highest priority thread
waiting on a cond_var, the value is equal to said thread's priorit

#2
In the struct thread in thread.c
struct thread
{  .
   .
   .
   int basePriority;
   int currPriority;
	 struct list_elem readyElem;
	 struct list_elem semaWaitElem;
   .
   .
   .
	 struct list locksHeld;
   struct lock *lockDesired;
}
All shown above were added for priority scheduling

int basePriority keeps track of a threads priority, excudling,
priority donation, so it can return to its origonal value after
a lock has been released.

int currPriority, keeps track of a threads curent highest priority
including donation, used for all scheduling.

struct lock *lockDesire keeps track of what lock, if any, a thread
is waiting on, thus allowing for easier priority donation.


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

We ensure the highest priority waiting on a semaphore wakes up by
purposefull waking it up. When sema_up is called we extract max
from the list of threads waiting on the given sema. The comparison
function we use returns A > B if A's priority > B's priority. Thus
the thread with the highest prioirty.

An important note here, and else where, is that we chose to keep
an ordered list of waiting threads for a given sema. Alternatively
we could have kept and ordered list and simply called list_pop_front
each sema_up. This seems to be faster. However, in a case where thread's
prioirties can be updated even while they are waiting (true more for
the advanced scheduler which also uses sema's of course) then there
will be a need to sort before list_pop_front. Given that, and the
extra time to insert_ordered as opposed to list_push_back. We chose
to go with an unsorted list.

Next, we ensure the highest priority thread waiting on a lock
wakes up first by having them utilize sema's. Ie., the origonal design
we were given used a semaphore to sleep threads under the hood. Since
the burden of sleeping threads and waking them up is actually left to
semaphores, and because our semaphores ensure the highest prioirty
thread wakes up first, locks ensure that the highest priority thread
waiting on them, wakes up first.

Finally, to discuss why the highest priority thread waiting on a condition 
variable wakes up first it is necessary to examine both cond_signal
and cond_broadcast separtely. It is important to note that we added
a priority field to the semaphore_elem struct. Whenever a thread enters
a cond_wait and then waits on a semaphore which is stored on the conditon
variables wait list, this semaphore_elem is given the priority of the thread.

When cond_signal is called, we iterate through the list of semaphores each
cond_variable contains. We then sema_up, ie., wake up the thread waiting,
the semaphore with the highest priority. Since this was donated by the
thread before it slept. This means that we are waking the thread with
the highest prioirty. 

Cond_broadcast, on the other hand, does not wake up a single thread.
Instead it wakes all threads. In this case the order of ready
threads will determine which thread 'wakes up'. Whichever thread
runs next will acquire the lock necessary to continue execution.
All other threads will be forced to wait on this lock, ie., 'to
sleep again until this lock is released'. 

Thus Cond_broadcast's thread waking order is dependent on the 
scheduler. In our case, the schduler wakes in order from highest
to lowest prioirty. Thus the thread with the highest prioirty
waiting on the condition variable will run next, thus acquiring 
the lock and thus be the only thread to run. In fact, since it is
the highest prioirty thread, it will, barring any decrease in its
prioirty, never relinquish control to the other waiting threads even
if releases the lock. That is, until it blocks or somehow other
becomes not ready. Or, if any of the other threads are somehow given
a higher prioirty.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a call to lock_acquire causes a prioirty donation the program enters
a while loop. The thread that wanted the lock will, as long as it has 
a higher priority then thread currently holding the lock, pass its prioirty
to the thread which currently hs the lock. This thread will then pass its
priority to the lock it is waiting on. This process continues until we 
arrive at either a) a thread which has a lock but also has a higher
prioirty then the thread that wants it, or b) a thread which is not
waiting on any lock. At this point the program returns to normal
execution.

We chose not to put a clamp on the number of nested donations our program
could handle. As each iteration the work required is simply a few derefences
we did not consider many of these operations unduly expensive. The alternative
which would be to clamp this number, would create a situation where our scheudler
was not working to move forward the goals of the highest prioirty thread. This
seemed contrary to priority scheudling.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release() is called and the lock has a higher-prioirty thread waiting
for it two actions occur in quick succesion. First, the current running thread's
prioirty will drop to its origonal value. In our design, the current thread 
running must have an artifically high priority. THe high priority thread would
have waited on said lock, causing priority donation to the thread holding the
lock, ie., the current thread running. Second, the thread calls thread_yield
via sema_up, which, barring the entrance of new higher priority threads, will
cause the higher-prioirty thread to run next.

When a thread releases a lock it owns, there are two parts to the event. First
the lock indicates to itself that it no longer has the lock. This means taking
the lock off the list of locks a thread owns. Next, the thread updates its own
priority. It loops through every lock it owns and records the highest prioirty of
a thread waiting on one them. Then it sets its own prioirty to the maximum of
that prioirty or its own base prioirty. Finally, the thread indicates to other
threads that it has released the lock, by calling sema_up which also calls 
thread_yield, switching the thread running if a higher priority thread is now
waiting to run. In this context, in case the current thread's prioirty has dropped.


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

One could have the case where one thread tries to change priority of another 
thread just before that thread calls thread_set_priority() internally.
This would result in a race condition where the receiving thread may
revert back to another priority right after a higher priority has been
donated, and as a result fail to run.

To mitigate this we include the code for thread_set_priority and schedule()
in synch.c and thread.c respectively.

alternatively we could have the case where a thread is switched in the middle
of donating its priority, and when it resumes it donates priority to a thread
that does not need to run.

to mitigate this we can put priority donation in sema_down.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Added to thread struct in thread.h

int niceness  
Keeps track of the nice value for each thread

int recentCPU
Keeps track of amount of CPU time each thread received recently


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4	4   0   0  62  61  59     A
 8	8   0   0  61  61  59     B
12	8   4   0  61  60  59     A
16	12  4   0  60  60  59     B
20      12  8   0  60  59  59     A
24	16  8   0  59  59  59     C
28	16  8   4  59  59  58     B
32	16  12  4  59  58  58     A
36	20  12  4  58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes, when two threads had the same priority. We resolved this conflict by
choosing the thread that was least recently run, which matches the round robin
behavior of our scheduler.  

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Advantages: We used one list instead of any array of 64 lists.

Disadvantages: 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

Clarity. We believe that adding a layer of abstraction makes our code
easier to read and easier to understand. It also decreases the size of
our code. A few of the operations on fixed points such as mulitplication
between two fixed-point numbers as well as the conversion from an integer to
a fixed point, are repated several times. The choice to create an API
for fixed points meant that we only would need to write these functions
one time and then apply them wherever necessary. Advantages 
of writing these functions only once are:  a) less chance of errors
b) increased flexibility of code. To change any fixed point operations
we need only change the .c file of the abstraction layer and volia,
the functions are changed every time they are used.


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

The first part was conceptually the easiest, and took the least time.
The second part took the majority of our time, and was conceptually the 
hardest part. The third part took longer than we expected, but was not 
conceptually hard. 

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Part 2 gave the greatest insight into threading.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

The TAs are all really helpful.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

